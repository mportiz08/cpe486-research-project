<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>natural user interfaces | research</title>
  <link rel="stylesheet" href="stylesheets/reset.css" type="text/css" media="screen">
  <link rel="stylesheet" href="stylesheets/style.css" type="text/css" media="screen">
</head>
<body>
  <div id="container">
    <header>
      <div class="fl">
        <h1>Natural User Interfaces</h1>
      </div>
      <div class="fr team-info">
        Team <strong>Kinecta<span class="kinect-purple">midi</span>a</strong><br />
        <em>research project</em><br />
        <em>cpe486 kurfess</em>
      </div>
      <div class="clear"></div>
    </header>
    <nav>
      <ul>
        <li><a href="history.html">History</a></li>
        <li><a class="selected" href="speech.html">Speech Recognition</a></li>
        <li><a href="gestures.html">Gestures</a></li>
        <li><a href="face.html">Facial Recognition</a></li>
      </ul>
    </nav>
    <section id="main">
      <div class="title">
        <h1>Speech Recognition</h1>
      </div>
      <div class="content">
        <h2>introduction</h2>

        <p>One of the most essential components of an effective <strong>natural user interface</strong> is <strong>speech recognition</strong> (also commonly referred to as voice recognition). In simple terms, <strong>speech recognition</strong> reflects the interaction between a person and some sort of computing device or system that involves the person using his/her voice to provide input to the system.</p>

        <p>Here is how the British-English dictionary <a href="http://www.macmillandictionary.com/dictionary/british/voice-recognition">defines</a> the term <strong>voice recognition</strong>:</p>

        <blockquote>
        <p>the ability of a computer to know the voice of a person speaking into it, so that only voices that the computer knows can use the system</p>
        </blockquote>

        <p>Let&#39;s look at a simple example of how this technology is being used by popular modern applications:</p>

        <p><img src="http://images.apple.com/iphone/features/images/siri_mean.jpg" alt="Siri"></p>

        <p>Siri (Speech Interpretation and Recognition Interface) is an application developed by Apple for the most recent version of their incredibly popular product, the iPhone. In this example of its use, a user simply asks their question to their phone with this application running on their phone, and the software responds by presenting the user with a list of appropriate restaurants along with reviews and maps for each one. Siri exemplifies the pinnacle of how <strong>speech recognition</strong> technology is being used in consumer products today.</p>

        <h2>technology</h2>

        <h3>overview</h3>

        <p>There are many components that make up the broad category of speech recognition. While the most important one is the actual recognition of speech itself, some of the lesser known of its components include <strong>speaker recognition</strong>, <strong>talk-through</strong> (also known as barge-in), <strong>word spotting</strong>, and <strong>decoy</strong>. First, I&#39;d like to take a look at the primary algorithms involved with speech recognition.</p>

        <h3>algorithms</h3>

        <h4>summary</h4>

        <p>The following is a very simplified summary of <a href="http://arxiv.org/pdf/cmp-lg/9608018.pdf">the speech recognition algorithm</a> as described by researchers from AT&amp;T:</p>

        <p>The basic problem that this algorithm is attempting to solve can be given by the following prompt:</p>

        <blockquote>
        <p>Given an utterance, find its most likely written transcription.</p>
        </blockquote>

        <p>The general solution for this involves transforming audio into weighted acyclic graphs that map possible interpretations of segments of the audio in the form of phonemes, phones, syllables, and words. The final written transcription for the audio is derived from the optimal path in this graph.</p>

        <p>The first step in this process is analyzing acoustic wave patterns from speech that at regular short intervals (for example, 10 milliseconds).</p>

        <h4>details</h4>

        <p>The basic process of recognizing speech is simplified as</p>

        <p>Speech <code>-&gt; 1. Feature Extraction -&gt; 2. Hypothesis Search -&gt;</code> Decoded Hypothesis</p>

        <p><img src="images/speech/algorithm-overview.png" alt="algorithm overview"></p>

        <ol>
        <li><p><strong>Feature Extraction</strong><br /><br />
        In the feature extraction phase, the software system extracts a set of 39 parameters describing the audio segment in a vector called a feature. The collection of features are often referred to as acoustic observations.</p></li>
        <li><p><strong>Hypothesis Search</strong><br /><br />
        The hypothesis search phase uses the feature vectors to analyze speech. Here, a lexicon is used in combination with a language and acoustic model. A lexicon is a list of every possible word with a unique pronunciation. Lexicons are often created with a certain task in mind to limit its size and therefore reduce the complexity of the algorithm. A language model computes the probability of a sequence of words and an acoustic model computes the probability of sequences of feature vectors. Together, these 3 things are used to create the weighted acyclic graph that was mentioned earlier. The nodes in this graph are word possibilities and the edges are weighted by the probabilities. Each path through the graph represents a possible sequence of words, and the final result is the sequence that has the best combined probability, which is the speech that the software has recognized.</p></li>
        </ol>

        <p>Here&#39;s a visual of the graph that the hypothesis search might use in an attempt to recognize the phrase <em>I move it very fast</em></p>

        <p><img src="images/speech/algorithm-graph.png" alt="algorithm graph"></p>

        <p>(source <a href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=9&amp;ved=0CHoQFjAI&amp;url=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D10.1.1.108.2469%26rep%3Drep1%26type%3Dpdf&amp;ei=3Wi9T_qKMavbiALJ3uGnDg&amp;usg=AFQjCNGAnyvKFVpi4N6VUNCCKE5AZoirMQ&amp;sig2=_e3iD8KUZlXO25J5JCpJjg">Padmanabhan, Picheny</a>)</p>

        <h3>related technology</h3>

        <h4>speaker recognition</h4>

        <p>Speaker recognition refers to the ability of the software to identify the individual who is speaking and is sometimes referred to as speaker identification. (source <a href="http://users.csc.calpoly.edu/%7Efkurfess/Courses/486/S12/Slides/486-S12-07-Speech.pdf">Kurfess</a>)</p>

        <p>This capability requires additional intelligence from the software system, so that it can accurately distinguish the voices of different people with minimal errors. In this way, this technology can be used in biometric systems, which are used to prevent access to unauthorized users. The most common use case for voice based biometric systems are as security measures. An example of this in the real life is the <a href="http://www.loquendo.com/en/products/speaker-verification/">Loquendo Speaker Verification</a>, which is a product used in a variety of businesses worldwide.</p>

        <p>For these types of applications, the term speaker identification is usually referred to as speaker <strong>verification</strong> in an attempt to better clarify the nature of the technology (security). Since a speaker verification system only needs to check for the correct user, it usually requires less complexity in the underlying algorithm. Speaker identification, on the other hand, requires more complexity in the underlying algorithm, because it has to attempt to actually match the voice to the appropriate user.</p>

        <p>There are two main types of speaker recognition:</p>

        <ol>
        <li><p><strong>text-dependent</strong>:<br />
        requires a matching text phrase to be spoken by the appropriate person to recognize the speaker</p></li>
        <li><p><strong>text-<em>in</em>dependent</strong>:<br />
        only requires the actual voice to match up</p></li>
        </ol>

        <p>(source: <a href="http://www.intechopen.com/books/biometrics/speaker-recognition">Beigi</a>)</p>

        <h4>talk-through</h4>

        <p>Talk-through, also known as barge-in, is simply the name for the technology that allows users of speech recognition systems to interrupt the system as it is prompting them for input. (source: <a href="http://users.csc.calpoly.edu/%7Efkurfess/Courses/486/S12/Slides/486-S12-07-Speech.pdf">Kurfess</a>)</p>

        <p>The most obvious example of this in use in the real world is from the automated phone systems of many large telephony companies. This is something that I have personal experience with. Recently, I was attempting to contact the support group for my cable company on the phone. This company used automated voice recognition software that began to list off the available options for me to speak. Before it had finished, I voiced the appropriate command for my system. In response, the system recognized my command and discontinued listing the other options, so that it could help me with the option that I chose.</p>

        <p>This type of voice technology is one that is becoming increasingly common with each passing year. Big companies like <a href="http://www.google.com/patents/US6563915">AT&amp;T</a>, <a href="http://www.google.com/patents/US6947895">Cisco</a>, <a href="http://www.google.com/patents/US20020184031">Hewlett Packard</a>, and <a href="http://www.google.com/patents/US6418216">IBM</a> all have patents that relate to barge-in enabled software.</p>

        <h4>word spotting</h4>

        <p>Word spotting is the ability of a voice recognition system to correctly identify specific words, even when surrounded by miscellaneous words that are irrelevant to the desired ones. (source: <a href="http://users.csc.calpoly.edu/%7Efkurfess/Courses/486/S12/Slides/486-S12-07-Speech.pdf">Kurfess</a>)</p>

        <p>This technology is essential for software that needs to analyze spoken words in an attempt to detect if a certain phrase was spoken or not. The U.S. government is one of the key users of word spotting technology. Homeland Security uses it in its surveillance of many different audio sources on a regular basis. In an attempt to help prevent terrorist plots, they analyze spoken audio in an automated fashion, using word spotting tuned for phrases that may indicate terrorist activity. (source: <a href="http://www.nscspeech.com/pdf/kws-whitepaper.pdf">Alon</a>)</p>

        <p>Other major users of word spotting technology are call centers. Some of them use software that is tuned to listen to phrases that indicate that a call is &quot;problematic&quot;, so that they can assist supervisors in finding and resolving them. Others use it to analyze their customers&#39; responses to the company itself after calls have taken place in order to better assess their effectiveness. (source <a href="http://www.nscspeech.com/pdf/kws-whitepaper.pdf">Alon</a>)</p>

        <h4>decoy</h4>

        <p>In the world of speech recognition, a decoy is defined as a word, phrase, or sound that is used to identify when the voice input has either stopped or been interrupted. (source: <a href="http://users.csc.calpoly.edu/%7Efkurfess/Courses/486/S12/Slides/486-S12-07-Speech.pdf">Kurfess</a>)</p>

        <p>Decoys are separated into 2 main categories:</p>

        <ul>
        <li>natural</li>
        <li>artificial</li>
        </ul>

        <p><em>Natural</em> decoys are inherent to the user and are most often associated with sounds having to do with hesitation or confusion on the user&#39;s part. <em>Artificial</em> decoys are not typically associated with the user&#39;s speech but with noises that are a product of the environment that he or she is in.</p>

        <p>Decoys are crucial for dictation software. Programs that offer dictation need to be able to reproduce a user&#39;s speech with text at a very accurate and consistent level. In order to do this, the software needs to be able to correctly identify accidental utterances like &quot;uh&quot; and &quot;um&quot;, so that it doesn&#39;t record these mistakes. By training the system to recognize these words as decoys, this problem can potentially be avoided.</p>

        <h2>kinect</h2>

        <h3>technical aspects</h3>

        <p>In addition to color, depth, and skeletal data, Microsoft&#39;s <a href="http://www.xbox.com/kinect/">Kinect</a> collects sound data, which allows the device to respond to voice commands. According to <a href="http://www.microsoft.com/en-us/news/press/2009/sep09/09-23tgsnatalpr.aspx">Microsoft&#39;s press release</a>, the Kinect (originally introduced as Project Natal) is equipped with a &quot;multi-array microphone&quot;, which handles processing the voice commands. The folks at <a href="http://www.ifixit.com/">iFixit</a> <a href="http://www.ifixit.com/Teardown/Microsoft-Kinect-Teardown/4066/">disassembled</a> the sensor and discovered that it contains 4 downward facing microphones that the array is composed of.</p>

        <p><img src="http://guide-images.ifixit.net/igi/VNIBCIuTyLIAUXqT.medium" alt="kinect microphone internals"></p>

        <p>The audio capabilities of the Kinect are as follows:</p>

        <ul>
        <li>acoustic noise suppression</li>
        <li>echo cancellation</li>
        <li>beam formation</li>
        </ul>

        <p>The Kinect sensor can detect audio that is within +-50 degrees in front of the sensor. Within that range, the microphone array can be pointed in 10 degree increments and can be programmatically directed. It also supports up to 20dB of ambient noise cancellation for mono sound input. Sound from behind the sensor will be suppressed up to 6dB. The default behavior of the device is to track the loudest audio source. (source: <a href="http://go.microsoft.com/fwlink/?LinkID=247735">Human Interface Guidelines</a>)</p>

        <p><img src="images/speech/kinect-audio-diagrams.png" alt="Kinect Audio Diagrams"></p>

        <p>Here&#39;s a <a href="http://support.xbox.com/en-US/kinect/voice/speech-recognition#d7e21da22ba34426bf8f46afc9028e79">link</a> to a listing of languages that the Kinect&#39;s speech recognition software currently recognizes and supports.</p>

        <p>To be used effectively, the device needs to be calibrated, so that it can better recognize voice commands. Also, the <a href="http://www.microsoft.com/en-us/kinectforwindows/">Kinect SDK</a> gives third party developers access to these audio features.</p>

        <h3>sdk</h3>

        <p>Microsoft provides programmatic access to the speech recognition capabilities of the Kinect with its software development kit. Its Runtime Language Pack has an acoustical model optimized for the device built in. To use it, developers need to create an instance of the Speech Recognizer Engine object. With that, they can load a customized grammar that contains the desired keywords to be detected. Then, developers need to implement an event handler that corresponds to the device when it may have recognized one of these words. This method will have access to a confidence level that represents how confident the sensor is that is has accurately recognized a word. Using this, developers can fine tune how sensitive their software is when dealing with voice commands.</p>

        <p>Here is a <a href="http://msdn.microsoft.com/en-us/library/hh378426.aspx">code sample</a> from Microsoft that demonstrates basic speech recognition using the sdk:</p>

        <pre><code>http://msdn.microsoft.com/en-us/library/hh378426.aspx

        using Microsoft.Speech.Recognition;
        using System;
        using System.Collections.Generic;
        using System.ComponentModel;
        using System.Data;
        using System.Drawing;
        using System.Linq;
        using System.Text;
        using System.Windows.Forms;

        namespace WindowsFormsApplication1
        {
          public partial class Form1 : Form
          {
            public Form1()
            {
              InitializeComponent();
            }

            private void Form1_Load(object sender, EventArgs e)
            {

              // Create a new SpeechRecognitionEngine instance.
              sre = new SpeechRecognitionEngine();

              // Create a simple grammar that recognizes &quot;red&quot;, &quot;green&quot;, or &quot;blue&quot;.
              Choices colors = new Choices();
              colors.Add(new string[] {&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;});

              // Create a GrammarBuilder object and append the Choices object.
              GrammarBuilder gb = new GrammarBuilder();
              gb.Append(colors);

              // Create the Grammar instance and load it into the speech recognition engine.
              Grammar g = new Grammar(gb);
              sre.LoadGrammar(g);

              // Register a handler for the SpeechRecognized event.
              sre.SpeechRecognized +=
                new EventHandler&lt;SpeechRecognizedEventArgs&gt;(sre_SpeechRecognized);
            }

            // Create a simple handler for the SpeechRecognized event.
            void sre_SpeechRecognized(object sender, SpeechRecognizedEventArgs e)
            {
              MessageBox.Show(e.Result.Text);
            }

            SpeechRecognitionEngine sre;
          }
        }
        </code></pre>

        <p>There are two main methods of handling voice input that software developers can choose to implement with the sdk: <strong>always on, active listening</strong> and <strong>keyword/trigger</strong>.</p>

        <h4>always on, active listening</h4>

        <p>In this mode, the sensor is constantly listening for voice input from the user. Microsoft recommends that this method if there are only a few number of words or phrases that your software will recognize. (source: <a href="http://go.microsoft.com/fwlink/?LinkID=247735">Human Interface Guidelines</a>)</p>

        <h4>keyword/trigger</h4>

        <p>In this mode, the sensor is only listening for a single word. If that word is recognized, then the sensor will wait for additional voice commands. An example of this is shown later with Microsoft&#39;s xbox 360 dashboard integration. (source: <a href="http://go.microsoft.com/fwlink/?LinkID=247735">Human Interface Guidelines</a>)</p>

        <h3>dashboard integration</h3>

        <p>Microsoft <a href="http://support.xbox.com/en-US/kinect/voice/speech-recognition">utilizes</a> the speech recognition capabilities of the Kinect by allowing Xbox 360 users to control their gaming consoles with their voice. On the main dashboard interface for the 360, a microphone icon is visible whenever users are able to use voice commands. If visible, users start by saying &quot;Xbox&quot; and then follow that with one of many different options that appear on the screen in a way that is context sensitive depending on the section of the dashboard that is being used. For example:</p>

        <blockquote>
        <p>&quot;Xbox&quot;<br />
        &quot;...play disc.&quot;</p>
        </blockquote>

        <p>Without any physical contact from the user, the Xbox will then load the software on the current disc.</p>

        <p><img src="http://nxeassets.xbox.com/shaxam/0201/6e/42/6e42ae2a-4ed8-4050-b5ea-3804036bdaeb.PNG?v=1#kinect-voice-recognition-m-m.PNG" alt="dashboar integration"></p>
        
      </div>
    </section>
  </div>
</body>
</html>